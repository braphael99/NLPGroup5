{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48346131-57ae-4d2b-9f6e-df6e9e046e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell if any import errors occur\n",
    "#! pip install datasets transformers\n",
    "#! pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a2f13c-6b75-4175-959f-88a010dc60d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.2\n"
     ]
    }
   ],
   "source": [
    "#Collecting all imports\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)\n",
    "#Transformers should be at least 4.11.0 required!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2981651e-ff6d-4d98-9519-4eb0cf2e0bd2",
   "metadata": {},
   "source": [
    "We must now load the dataset from our local JSON files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74b3a79-e6d1-4eda-903b-64123e120ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The ranger and the rustler both were riding horses that galloped at the same speed.  The rustler left at 01:00 where as the ranger left at 0500 hours. Who has traveled further?? \n",
      "O1: the ranger\n",
      "O2: the rustler\n",
      "A: Option 2\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset as train, validation, and test.\n",
    "#We use the dev data as validation.\n",
    "\n",
    "datasets = load_dataset(\"json\", data_files={'train':'QQA Data/QQA_train.json', \n",
    "                                           'validation':'QQA Data/QQA_dev.json', \n",
    "                                           'test':'QQA Data/QQA_test.json'})\n",
    "\n",
    "#Printing the dataset contents\n",
    "print('Q: ' + datasets['train'][0]['question'])\n",
    "print('O1: ' + datasets['train'][0]['Option1'])\n",
    "print('O2: ' + datasets['train'][0]['Option2'])\n",
    "print('A: ' + datasets['train'][0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fcaace2-8030-4319-bc42-5ed145b06744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_sci_10E', 'question', 'Option2', 'question_char', 'answer', 'Option1', 'question_mask', 'type', 'question_sci_10E_char'],\n",
       "        num_rows: 564\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_sci_10E', 'question', 'Option2', 'question_char', 'answer', 'Option1', 'question_mask', 'type', 'question_sci_10E_char'],\n",
       "        num_rows: 81\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question_sci_10E', 'question', 'Option2', 'question_char', 'answer', 'Option1', 'question_mask', 'type', 'question_sci_10E_char'],\n",
       "        num_rows: 162\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print dataset structure\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3baa20a-caaf-4610-924e-676a7be967b6",
   "metadata": {},
   "source": [
    "The imported data is of the form:\n",
    "\n",
    "{\"question\": \"Jame's mother has a photo of Jane standing at a height of 14 inches, whereas a mountain appears to have height of 26 cm. It looks that way because? \", \"Option1\": \"the mountain was farther away\", \"Option2\": \"Jane was farther away\", \"answer\": \"Option 2\", \"type\": \"Type_3\", \"question_sci_10E\": \"Jame's mother has a photo of Jane standing at a height of 1.4000000000E+01 inches, whereas a mountain appears to have height of 2.6000000000E+01 cm. It looks that way because? \", \"question_char\": \"Jame's mother has a photo of Jane standing at a height of 1 4 inches, whereas a mountain appears to have height of 2 6 cm. It looks that way because? \", \"question_sci_10E_char\": \"Jame's mother has a photo of Jane standing at a height of 1 . 4 0 0 0 0 0 0 0 0 0 E + 0 1 inches, whereas a mountain appears to have height of 2 . 6 0 0 0 0 0 0 0 0 0 E + 0 1 cm. It looks that way because? \", \"question_mask\": \"Jame's mother has a photo of Jane standing at a height of [Num] inches, whereas a mountain appears to have height of [Num] cm. It looks that way because? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9435d4a-07ed-4765-9b9e-f164e90c573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get rid of the other columns\n",
    "datasets = datasets.remove_columns(['question_char', 'question_sci_10E',\n",
    "                         'question_sci_10E_char',\n",
    "                         'question_mask', 'type',])\n",
    "\n",
    "#We now only have a question, answer, and 2 options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc7f584-de67-4956-b749-06e886d81b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming for compatibility later\n",
    "datasets = datasets.rename_column('answer', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f5147-a707-4d31-8bd7-98a2f593ba82",
   "metadata": {},
   "source": [
    "For evaluation, see https://huggingface.co/spaces/evaluate-metric/accuracy\n",
    "\n",
    "For fine-tuning, see: https://huggingface.co/docs/transformers/en/training#train-with-pytorch-trainer\n",
    "\n",
    "For handing multiple choice, see https://huggingface.co/docs/transformers/en/tasks/multiple_choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a3e901-9c1c-4676-80d9-0f5f75404d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set correct answer to an integer. \n",
    "\n",
    "def set_labels(example):\n",
    "    #print(example)\n",
    "    example[\"label\"] = int(example[\"label\"][-1]) - 1\n",
    "    return example\n",
    "\n",
    "datasets = datasets.map(set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f364ab20-e1da-4ca8-a7d0-a3bbb6d5a949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A race car and a pickup both drove on the highway at the same speed. The driver of the race car got tired and parked after 29 mins, while the driver of the pickup ran for 43 mins. Which vehicle ultimately went the greater distance?? ',\n",
       " 'Option2': 'pickup',\n",
       " 'label': 1,\n",
       " 'Option1': 'race car'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e251187a-8ecc-4a47-9e5a-9ad0df35ae4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1037, 4049, 1012, 102, 2023, 2003, 1037, 4946, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "tokenizer(\"This is a boat.\", \"This is a plane.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a6f8fda-78af-4194-8f2d-a7cf9c2854d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_names = [\"Option1\", \"Option2\"]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Repeat each first sentence two times to go with the two possibilities of second sentences.\n",
    "    first_sentences = [[context] * 2 for context in examples[\"question\"]]\n",
    "    # Grab all second sentences possible for each context.\n",
    "    question_headers = examples[\"question\"]\n",
    "    second_sentences = [[f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)]\n",
    "    \n",
    "    # Flatten everything\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    # Un-flatten\n",
    "    return {k: [v[i:i+2] for i in range(0, len(v), 2)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b34511-0436-4d34-8a75-d70dbbc41ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2 [89, 90]\n"
     ]
    }
   ],
   "source": [
    "examples = datasets[\"train\"][:5]\n",
    "features = preprocess_function(examples)\n",
    "print(len(features[\"input_ids\"]), len(features[\"input_ids\"][0]), [len(x) for x in features[\"input_ids\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc11911-b731-4c65-8120-7a4ad4aa778d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] a tank weighs around 63 tons. a toy car weighs 1. 5 kg. because of this? [SEP] a tank weighs around 63 tons. a toy car weighs 1. 5 kg. because of this? the tank will speed up faster than the toy car [SEP]',\n",
       " '[CLS] a tank weighs around 63 tons. a toy car weighs 1. 5 kg. because of this? [SEP] a tank weighs around 63 tons. a toy car weighs 1. 5 kg. because of this? the toy car will speed up faster than the tank [SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 3\n",
    "[tokenizer.decode(features[\"input_ids\"][idx][i]) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46bef0f7-7a5d-43c5-9fcb-fd4ec41cf9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_datasets = datasets.map(preprocess_function, batched=True)\n",
    "#print(encoded_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f7c4b4f-c37a-423c-b5aa-f6aa6d56f968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b70edf3e-ddec-4110-9888-5dfaa6b89cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e1bd0e-18f9-4c18-8404-ec91609ac08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing stuff. Be sure to create encoded_datasets if you're running this. \n",
    "#accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "#features = [{k: v for k, v in encoded_datasets[\"train\"][i].items() if k in accepted_keys} for i in range(10)]\n",
    "#batch = DataCollatorForMultipleChoice(tokenizer)(features)\n",
    "#[tokenizer.decode(batch[\"input_ids\"][8][i].tolist()) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02763447-68f9-448c-93fc-b2f030126e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the F1 score metric to evaluate our predictions. \n",
    "\n",
    "#Old evaluator.\n",
    "'''\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}\n",
    "'''\n",
    "\n",
    "#New evaluator.\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    \n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": f1_score(preds, label_ids, average='micro').astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b93099c3-efa5-4dc0-832b-2dad3d393318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The ranger and the rustler both were riding horses that galloped at the same speed.  The rustler left at 01:00 where as the ranger left at 0500 hours. Who has traveled further?? \n",
      "O1: the ranger\n",
      "O2: the rustler\n",
      "A: Option 2\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'Option2', 'label', 'Option1'],\n",
      "        num_rows: 564\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'Option2', 'label', 'Option1'],\n",
      "        num_rows: 81\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'Option2', 'label', 'Option1'],\n",
      "        num_rows: 162\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# A cell to reset the dataset if/when problems occur with it.\n",
    "# Probably good to run this if you ran the above cells, since there's a lot of testing stuff for one model\n",
    "\n",
    "datasets = load_dataset(\"json\", data_files={'train':'QQA Data/QQA_train.json', \n",
    "                                           'validation':'QQA Data/QQA_dev.json', \n",
    "                                           'test':'QQA Data/QQA_test.json'})\n",
    "\n",
    "#Printing the dataset contents\n",
    "print('Q: ' + datasets['train'][0]['question'])\n",
    "print('O1: ' + datasets['train'][0]['Option1'])\n",
    "print('O2: ' + datasets['train'][0]['Option2'])\n",
    "print('A: ' + datasets['train'][0]['answer'])\n",
    "\n",
    "#Let's get rid of the other columns\n",
    "datasets = datasets.remove_columns(['question_char', 'question_sci_10E',\n",
    "                         'question_sci_10E_char',\n",
    "                         'question_mask', 'type',])\n",
    "\n",
    "#We now only have a question, answer, and 2 options.\n",
    "\n",
    "datasets = datasets.rename_column('answer', 'label')\n",
    "\n",
    "def set_labels(example):\n",
    "    #print(example)\n",
    "    example[\"label\"] = int(example[\"label\"][-1]) - 1\n",
    "    return example\n",
    "\n",
    "datasets = datasets.map(set_labels)\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1187dac7-b9c6-4c60-b26f-b28ba10d4f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function. Duplicate and change model name for other models.\n",
    "\n",
    "modelName = \"\"\n",
    "\n",
    "def autoTrain(model_name = 'bert-base-uncased', batch_size = 16):\n",
    "    global model\n",
    "    global tokenizer\n",
    "    global modelName\n",
    "    modelName = model_name\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    \n",
    "    encoded_datasets = datasets.map(preprocess_function, batched=True)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        f\"{model_name}-finetuned-QQA\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=encoded_datasets[\"train\"],\n",
    "        eval_dataset=encoded_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForMultipleChoice(tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a012d073-cddd-4ae3-ab6c-aefc648d22cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'Option2', 'label', 'Option1'],\n",
      "    num_rows: 162\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Cell for testing\n",
    "print(datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a1871d6-93e8-4158-8875-093decb96888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example prediction\n",
    "\n",
    "prompt = \"I have 5 bagels and Joe has 2. Who has more bagels?\"\n",
    "candidate1 = \"Me\"\n",
    "candidate2 = \"Joe\"\n",
    "\n",
    "\n",
    "inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"pt\", padding=True)\n",
    "labels = torch.tensor(0).unsqueeze(0)\n",
    "outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_class = logits.argmax().item()\n",
    "predicted_class\n",
    "\n",
    "#Note that it will output a 0 or 1, where 0 = Option 1 and 1 = Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23131c0b-4967-462a-a357-11bf5ce60a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eval the most recently trained model\n",
    "def evaluate_hf_model():\n",
    "\n",
    "    global model\n",
    "    global tokenizer\n",
    "\n",
    "            \n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Get predictions, and save corresponding reference (if we were using the whole dataset, we wouldn't need this step)\n",
    "    for ex in datasets[\"test\"]:\n",
    "\n",
    "        #Based on the above cell, get \n",
    "        prompt = ex['question']\n",
    "        candidate1 = ex['Option1']\n",
    "        candidate2 = ex['Option2']\n",
    "        \n",
    "        inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"pt\", padding=True)\n",
    "        labels = torch.tensor(0).unsqueeze(0)\n",
    "        outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        predicted_class = logits.argmax().item()\n",
    "        predicted_class\n",
    "\n",
    "\n",
    "        predictions.append(predicted_class)\n",
    "        references.append(ex['label'])\n",
    "\n",
    "    # Compute metrics\n",
    "    global modelName\n",
    "    print('Performance of {} : {}'.format(modelName, f1_score(predictions, references, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94f6b934-2ac4-4ed8-9851-6190e2abc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEval(model_name, batch_size):\n",
    "    autoTrain(model_name, batch_size)\n",
    "    #eval_hf_model evaluates whatever model we just trained because of global variables. \n",
    "    #As such we have it eval after training.\n",
    "    evaluate_hf_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99a7c809-1c79-404a-bf3e-27264b648617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/patrick/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/108 33:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693723</td>\n",
       "      <td>0.530864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693442</td>\n",
       "      <td>0.481481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693523</td>\n",
       "      <td>0.469136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of bert-base-uncased : 0.5\n"
     ]
    }
   ],
   "source": [
    "#Training and evaluating bert-base-uncased\n",
    "trainAndEval(\"bert-base-uncased\", 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ea691-1d91-4377-b132-cfea9edd10e5",
   "metadata": {},
   "source": [
    "# Comparison:\n",
    "\n",
    "The models that we evaluated were the following:\n",
    "- bert-base-uncased, trained on a large corpus of unlabeled data with the objective of masked language modeling and next sentence prediction. Because there are no \"special\" features in this version of BERT, and because of its ubiquity, it is used as a baseline for other models to be compared to in this project. https://huggingface.co/google-bert/bert-base-uncased\n",
    "\n",
    "All models were fine-tuned on the QQA dataset for 3 epochs with a batch size of 16, and evaluated using the evaluate_hf_model() function afterward. For evaluation, the scikit F1 micro score was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3818375-0070-4401-a42a-dee04698704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the name and accuracy to avoid having to redo everything all the time.\n",
    "#Also good for graphing. \n",
    "\n",
    "modelNameAcc = [\n",
    "    [\"bart-base-uncased\", 0.5]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21a7cafa-d78a-4bfb-8335-3727ac3f37fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot model names, accuracies, on a graph for easy comparison\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#X points for plotting\n",
    "xpoints = np.linspace(1, len(modelNameAcc), len(modelNameAcc))\n",
    "ypoints = np.zeros(len(modelNameAcc))\n",
    "\n",
    "countModels = 0\n",
    "for k in range(len(modelNameAcc)):\n",
    "    model, acc = modelNameAcc[k]\n",
    "    countModels += 1\n",
    "    ypoints[k] = acc\n",
    "\n",
    "#Set plot and plot\n",
    "plt.xticks([1], ['bert-base-uncased'])\n",
    "plt.scatter(xpoints, ypoints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0711f-7fc9-4784-81c1-1537b1eccdce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
