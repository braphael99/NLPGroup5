{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48346131-57ae-4d2b-9f6e-df6e9e046e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell if any import errors occur\n",
    "#! pip install datasets transformers\n",
    "#! pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a2f13c-6b75-4175-959f-88a010dc60d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.2\n"
     ]
    }
   ],
   "source": [
    "#Collecting all imports\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)\n",
    "#Transformers should be at least 4.11.0 required!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2981651e-ff6d-4d98-9519-4eb0cf2e0bd2",
   "metadata": {},
   "source": [
    "We must now load the dataset from our local JSON files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74b3a79-e6d1-4eda-903b-64123e120ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The ranger and the rustler both were riding horses that galloped at the same speed.  The rustler left at 01:00 where as the ranger left at 0500 hours. Who has traveled further?? \n",
      "O1: the ranger\n",
      "O2: the rustler\n",
      "A: Option 2\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset as train, validation, and test.\n",
    "#We use the dev data as validation.\n",
    "\n",
    "datasets = load_dataset(\"json\", data_files={'train':'QQA Data/QQA_train.json', \n",
    "                                           'validation':'QQA Data/QQA_dev.json', \n",
    "                                           'test':'QQA Data/QQA_test.json'})\n",
    "\n",
    "#Printing the dataset contents\n",
    "print('Q: ' + datasets['train'][0]['question'])\n",
    "print('O1: ' + datasets['train'][0]['Option1'])\n",
    "print('O2: ' + datasets['train'][0]['Option2'])\n",
    "print('A: ' + datasets['train'][0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fcaace2-8030-4319-bc42-5ed145b06744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question_sci_10E', 'question', 'Option2', 'question_char', 'answer', 'Option1', 'question_mask', 'type', 'question_sci_10E_char'],\n",
       "        num_rows: 564\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question_sci_10E', 'question', 'Option2', 'question_char', 'answer', 'Option1', 'question_mask', 'type', 'question_sci_10E_char'],\n",
       "        num_rows: 81\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question_sci_10E', 'question', 'Option2', 'question_char', 'answer', 'Option1', 'question_mask', 'type', 'question_sci_10E_char'],\n",
       "        num_rows: 162\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print dataset structure\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3baa20a-caaf-4610-924e-676a7be967b6",
   "metadata": {},
   "source": [
    "The imported data is of the form:\n",
    "\n",
    "{\"question\": \"Jame's mother has a photo of Jane standing at a height of 14 inches, whereas a mountain appears to have height of 26 cm. It looks that way because? \", \"Option1\": \"the mountain was farther away\", \"Option2\": \"Jane was farther away\", \"answer\": \"Option 2\", \"type\": \"Type_3\", \"question_sci_10E\": \"Jame's mother has a photo of Jane standing at a height of 1.4000000000E+01 inches, whereas a mountain appears to have height of 2.6000000000E+01 cm. It looks that way because? \", \"question_char\": \"Jame's mother has a photo of Jane standing at a height of 1 4 inches, whereas a mountain appears to have height of 2 6 cm. It looks that way because? \", \"question_sci_10E_char\": \"Jame's mother has a photo of Jane standing at a height of 1 . 4 0 0 0 0 0 0 0 0 0 E + 0 1 inches, whereas a mountain appears to have height of 2 . 6 0 0 0 0 0 0 0 0 0 E + 0 1 cm. It looks that way because? \", \"question_mask\": \"Jame's mother has a photo of Jane standing at a height of [Num] inches, whereas a mountain appears to have height of [Num] cm. It looks that way because? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9435d4a-07ed-4765-9b9e-f164e90c573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get rid of the other columns\n",
    "datasets = datasets.remove_columns(['question_char', 'question_sci_10E',\n",
    "                         'question_sci_10E_char',\n",
    "                         'question_mask', 'type',])\n",
    "\n",
    "#We now only have a question, answer, and 2 options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc7f584-de67-4956-b749-06e886d81b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming for compatibility later\n",
    "datasets = datasets.rename_column('answer', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f5147-a707-4d31-8bd7-98a2f593ba82",
   "metadata": {},
   "source": [
    "For evaluation, see https://huggingface.co/spaces/evaluate-metric/accuracy\n",
    "\n",
    "For fine-tuning, see: https://huggingface.co/docs/transformers/en/training#train-with-pytorch-trainer\n",
    "\n",
    "For handing multiple choice, see https://huggingface.co/docs/transformers/en/tasks/multiple_choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a3e901-9c1c-4676-80d9-0f5f75404d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set correct answer to an integer. \n",
    "\n",
    "def set_labels(example):\n",
    "    #print(example)\n",
    "    example[\"label\"] = int(example[\"label\"][-1]) - 1\n",
    "    return example\n",
    "\n",
    "datasets = datasets.map(set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f364ab20-e1da-4ca8-a7d0-a3bbb6d5a949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A race car and a pickup both drove on the highway at the same speed. The driver of the race car got tired and parked after 29 mins, while the driver of the pickup ran for 43 mins. Which vehicle ultimately went the greater distance?? ',\n",
       " 'Option2': 'pickup',\n",
       " 'label': 1,\n",
       " 'Option1': 'race car'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e251187a-8ecc-4a47-9e5a-9ad0df35ae4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1037, 4049, 1012, 102, 2023, 2003, 1037, 4946, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "tokenizer(\"This is a boat.\", \"This is a plane.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a6f8fda-78af-4194-8f2d-a7cf9c2854d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_names = [\"Option1\", \"Option2\"]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Repeat each first sentence two times to go with the two possibilities of second sentences.\n",
    "    first_sentences = [[context] * 2 for context in examples[\"question\"]]\n",
    "    # Grab all second sentences possible for each context.\n",
    "    question_headers = examples[\"question\"]\n",
    "    second_sentences = [[f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)]\n",
    "    \n",
    "    # Flatten everything\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    # Un-flatten\n",
    "    return {k: [v[i:i+2] for i in range(0, len(v), 2)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b34511-0436-4d34-8a75-d70dbbc41ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2 [89, 90]\n"
     ]
    }
   ],
   "source": [
    "examples = datasets[\"train\"][:5]\n",
    "features = preprocess_function(examples)\n",
    "print(len(features[\"input_ids\"]), len(features[\"input_ids\"][0]), [len(x) for x in features[\"input_ids\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc11911-b731-4c65-8120-7a4ad4aa778d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] a tank weighs around 63 tons. a toy car weighs 1. 5 kg. because of this? [SEP] a tank weighs around 63 tons. a toy car weighs 1. 5 kg. because of this? the tank will speed up faster than the toy car [SEP]',\n",
       " '[CLS] a tank weighs around 63 tons. a toy car weighs 1. 5 kg. because of this? [SEP] a tank weighs around 63 tons. a toy car weighs 1. 5 kg. because of this? the toy car will speed up faster than the tank [SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 3\n",
    "[tokenizer.decode(features[\"input_ids\"][idx][i]) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46bef0f7-7a5d-43c5-9fcb-fd4ec41cf9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_datasets = datasets.map(preprocess_function, batched=True)\n",
    "#print(encoded_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f7c4b4f-c37a-423c-b5aa-f6aa6d56f968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b70edf3e-ddec-4110-9888-5dfaa6b89cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e1bd0e-18f9-4c18-8404-ec91609ac08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing stuff. Be sure to create encoded_datasets if you're running this. \n",
    "#accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "#features = [{k: v for k, v in encoded_datasets[\"train\"][i].items() if k in accepted_keys} for i in range(10)]\n",
    "#batch = DataCollatorForMultipleChoice(tokenizer)(features)\n",
    "#[tokenizer.decode(batch[\"input_ids\"][8][i].tolist()) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02763447-68f9-448c-93fc-b2f030126e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the F1 score metric to evaluate our predictions. \n",
    "\n",
    "#Old evaluator.\n",
    "'''\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}\n",
    "'''\n",
    "\n",
    "#New evaluator.\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    \n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "    print(preds)\n",
    "    return {\"accuracy\": f1_score(preds, label_ids, average='micro').astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfe8667b-8988-49cf-8fb4-1d2392ed2427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrainer = Trainer(\\n    model,\\n    args,\\n    train_dataset=encoded_datasets[\"train\"],\\n    eval_dataset=encoded_datasets[\"validation\"],\\n    tokenizer=tokenizer,\\n    data_collator=DataCollatorForMultipleChoice(tokenizer),\\n    compute_metrics=compute_metrics,\\n)\\ntrainer.train()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_datasets[\"train\"],\n",
    "    eval_dataset=encoded_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b93099c3-efa5-4dc0-832b-2dad3d393318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: The ranger and the rustler both were riding horses that galloped at the same speed.  The rustler left at 01:00 where as the ranger left at 0500 hours. Who has traveled further?? \n",
      "O1: the ranger\n",
      "O2: the rustler\n",
      "A: Option 2\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'Option2', 'label', 'Option1'],\n",
      "        num_rows: 564\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'Option2', 'label', 'Option1'],\n",
      "        num_rows: 81\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'Option2', 'label', 'Option1'],\n",
      "        num_rows: 162\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# A cell to reset the dataset if/when problems occur with it.\n",
    "# Probably good to run this if you ran the above cells, since there's a lot of testing stuff for one model\n",
    "\n",
    "datasets = load_dataset(\"json\", data_files={'train':'QQA Data/QQA_train.json', \n",
    "                                           'validation':'QQA Data/QQA_dev.json', \n",
    "                                           'test':'QQA Data/QQA_test.json'})\n",
    "\n",
    "#Printing the dataset contents\n",
    "print('Q: ' + datasets['train'][0]['question'])\n",
    "print('O1: ' + datasets['train'][0]['Option1'])\n",
    "print('O2: ' + datasets['train'][0]['Option2'])\n",
    "print('A: ' + datasets['train'][0]['answer'])\n",
    "\n",
    "#Let's get rid of the other columns\n",
    "datasets = datasets.remove_columns(['question_char', 'question_sci_10E',\n",
    "                         'question_sci_10E_char',\n",
    "                         'question_mask', 'type',])\n",
    "\n",
    "#We now only have a question, answer, and 2 options.\n",
    "\n",
    "datasets = datasets.rename_column('answer', 'label')\n",
    "\n",
    "def set_labels(example):\n",
    "    #print(example)\n",
    "    example[\"label\"] = int(example[\"label\"][-1]) - 1\n",
    "    return example\n",
    "\n",
    "datasets = datasets.map(set_labels)\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1187dac7-b9c6-4c60-b26f-b28ba10d4f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function. Duplicate and change model name for other models.\n",
    "\n",
    "modelName = \"\"\n",
    "\n",
    "def autoTrain(model_name = 'bert-base-uncased', batch_size = 16):\n",
    "    global model\n",
    "    global tokenizer\n",
    "    global modelName\n",
    "    modelName = model_name\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    \n",
    "    encoded_datasets = datasets.map(preprocess_function, batched=True)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        f\"{model_name}-finetuned-QQA\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=encoded_datasets[\"train\"],\n",
    "        eval_dataset=encoded_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForMultipleChoice(tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e47cce3c-cb65-4109-a9d7-9a13e3aaafd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/108 : < :, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mautoTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#autoTrain('deepset/roberta-base-squad2', 128)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 31\u001b[0m, in \u001b[0;36mautoTrain\u001b[0;34m(model_name, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-finetuned-QQA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     evaluation_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     22\u001b[0m     model,\n\u001b[1;32m     23\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/trainer.py:2744\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2742\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2743\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "autoTrain(\"bert-base-uncased\", 16)\n",
    "\n",
    "#autoTrain('deepset/roberta-base-squad2', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a012d073-cddd-4ae3-ab6c-aefc648d22cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'Option2', 'label', 'Option1'],\n",
      "    num_rows: 162\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Cell for testing\n",
    "print(datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a1871d6-93e8-4158-8875-093decb96888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Will tune this to have it evaluate. \n",
    "\n",
    "prompt = \"I have 5 bagels and Joe has 2. Who has more bagels?\"\n",
    "candidate1 = \"Me\"\n",
    "candidate2 = \"Joe\"\n",
    "\n",
    "\n",
    "inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"pt\", padding=True)\n",
    "labels = torch.tensor(0).unsqueeze(0)\n",
    "outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_class = logits.argmax().item()\n",
    "predicted_class\n",
    "\n",
    "#Note that it will output a 0 or 1, where 0 = Option 1 and 1 = Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23131c0b-4967-462a-a357-11bf5ce60a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of amogus : 0.4567901234567901\n"
     ]
    }
   ],
   "source": [
    "#Model the most recently trained model\n",
    "def evaluate_hf_model():\n",
    "\n",
    "    global model\n",
    "    global tokenizer\n",
    "\n",
    "            \n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Get predictions, and save corresponding reference (if we were using the whole dataset, we wouldn't need this step)\n",
    "    for ex in datasets[\"test\"]:\n",
    "\n",
    "        #Based on the above cell, get \n",
    "        prompt = ex['question']\n",
    "        candidate1 = ex['Option1']\n",
    "        candidate2 = ex['Option2']\n",
    "        \n",
    "        inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"pt\", padding=True)\n",
    "        labels = torch.tensor(0).unsqueeze(0)\n",
    "        outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        predicted_class = logits.argmax().item()\n",
    "        predicted_class\n",
    "\n",
    "\n",
    "        predictions.append(predicted_class)\n",
    "        references.append(ex['label'])\n",
    "\n",
    "    # Compute metrics\n",
    "    global modelName\n",
    "    print('Performance of {} : {}'.format(modelName, f1_score(predictions, references, average='micro')))\n",
    "\n",
    "evaluate_hf_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3818375-0070-4401-a42a-dee04698704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the name and accuracy to avoid having to re-evaluate everything all the time.\n",
    "#Also good for graphing. \n",
    "\n",
    "modelNameAcc = {\n",
    "    [\"name\", \"evaluation accuracy\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7cafa-d78a-4bfb-8335-3727ac3f37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot model names, accuracies, on a graph for easy comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
