{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1bedf3",
   "metadata": {},
   "source": [
    "# NLPGroup5: Comparing Pre-Trained Modelsâ€™ Performance on QQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe99caa",
   "metadata": {},
   "source": [
    "### Part 1: Setting up for Evaluation\n",
    "To get started, we are importing all the necessary libraries to perform our evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11774f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\braph\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\braph\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "#Collecting all imports\n",
    "#Transformers should be at least 4.11.0 required!\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForMultipleChoice, TrainingArguments, Trainer, AutoModelForQuestionAnswering\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import transformers\n",
    "import json\n",
    "from evaluate import load\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62707ed",
   "metadata": {},
   "source": [
    "Now we are loading our dataset in from our JSON files and manipulating to fit our evaluation process. See the README for where to find these dataset files from the NumEval @ SemEval 2024 site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1e837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"json\", data_files={'train':'NLPGroup5/Project/QQA_Data/QQA_train.json', \n",
    "                                           'validation':'NLPGroup5/Project/QQA_Data/QQA_dev.json', \n",
    "                                           'test':'NLPGroup5/Project/QQA_Data/QQA_test.json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81929eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are dropping unnecessary columns for brevity\n",
    "datasets = datasets.remove_columns(['question_char', 'question_sci_10E',\n",
    "                         'question_sci_10E_char',\n",
    "                         'question_mask', 'type',])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e365cc86",
   "metadata": {},
   "source": [
    "We are evaluating our models on \"Exact Match\" from Huggingface. Essentially we want our model to be <u>completely</u> accurate or else our predicted answers do not count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e9157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match = load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da794317",
   "metadata": {},
   "source": [
    "### Part 2: Evaluation Function and Variable Initialization\n",
    "Here we are defining our evaluation function. Here we set up our pretrained models for question answering and tokenize using an AutoTokenizer. We are also using a question answering pipeline from Huggingface using our model and tokenizer. We manipulate our dataset to fit the expected format of our pipeline. We then generate predictions using the pipeline and append those along with our references (correct answers). Finally we compute our comparison metrics (exact match) and print those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf8cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intitializng our model name variable and performance list\n",
    "model_name = ''\n",
    "modelPerformanceList = []\n",
    "modelNames = []\n",
    "\n",
    "def evaluate_hf_model(model_name, dataset):\n",
    "    global modelPerformance\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)       # Initialize the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)                   # Initialize the tokenizer\n",
    "    \n",
    "    #initialize our pipeline with our input model\n",
    "    processor = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    #format our dataset\n",
    "    def dataset_generator(dataset):\n",
    "        for ex in dataset:\n",
    "            #Here we clean the option answer to use later for \"context\"\n",
    "            cleansedAnswer = ex['answer'].replace(\" \", \"\")\n",
    "            yield (ex,\n",
    "                {'question' : ex['question'], 'context': ex[cleansedAnswer]})\n",
    "    #initialize predictions and references lists        \n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Get predictions\n",
    "    for ex in tqdm(dataset_generator(datasets[dataset]), total=len(datasets[dataset])):\n",
    "        predictions.append(processor(ex[1])['answer'])\n",
    "\n",
    "        #Appending our answer which will be our reference\n",
    "        references.append(ex[0][ex[0]['answer'].replace(\" \", \"\")])\n",
    "\n",
    "    # Compute metrics\n",
    "    modelPerformance = exact_match.compute(predictions=predictions, references=references)\n",
    "    print('Performance of {} : {}'.format(model_name, modelPerformance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe099aeb",
   "metadata": {},
   "source": [
    "### Part 3:Evaluation on Train Dataset\n",
    "In this section we are evaluating performances for our models on our \"Train\" dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c592448",
   "metadata": {},
   "source": [
    "#### Baseline Models\n",
    "Here we are evaluating our \"Baseline\" models like BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7817382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0c93ce4f5d4feab1174b9663533261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of bert-base-uncased : {'exact_match': 0.4875886524822695}\n",
      "bert-base-uncased performance saved: {'exact_match': 0.4875886524822695}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adee8f798d9844b1adf8a6ab6c01a93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of distilbert/distilbert-base-uncased : {'exact_match': 0.4627659574468085}\n",
      "distilbert/distilbert-base-uncased performance saved: {'exact_match': 0.4627659574468085}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873e3deccee84947a10f71702c92a8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of dccuchile/bert-base-spanish-wwm-cased : {'exact_match': 0.6063829787234043}\n",
      "dccuchile/bert-base-spanish-wwm-cased performance saved: {'exact_match': 0.6063829787234043}\n"
     ]
    }
   ],
   "source": [
    "#We are evaluating on our \"train\" dataset in this section\n",
    "datasetEval = 'train'\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "evaluate_hf_model(model_name, datasetEval)\n",
    "modelNames.append(model_name)\n",
    "modelPerformanceList.append(modelPerformance)\n",
    "print('{} performance saved: {}'.format(model_name, modelPerformance))\n",
    "\n",
    "model_name = 'distilbert/distilbert-base-uncased'\n",
    "evaluate_hf_model(model_name, datasetEval)\n",
    "modelNames.append(model_name)\n",
    "modelPerformanceList.append(modelPerformance)\n",
    "print('{} performance saved: {}'.format(model_name, modelPerformance))\n",
    "\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "evaluate_hf_model(model_name, datasetEval)\n",
    "modelNames.append(model_name)\n",
    "modelPerformanceList.append(modelPerformance)\n",
    "print('{} performance saved: {}'.format(model_name, modelPerformance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f20cbe",
   "metadata": {},
   "source": [
    "#### SQuAD Models\n",
    "Here we are evaluating our \"SQuAD\" models like RoBERTa and Dynamic TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b90e476b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc665c207aa439f9318097b70cdf63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of deepset/roberta-base-squad2 : {'exact_match': 0.7801418439716312}\n",
      "deepset/roberta-base-squad2 performance saved: {'exact_match': 0.7801418439716312}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98030f15a4f34ffc8716f5eb0000ac31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Intel/dynamic_tinybert : {'exact_match': 0.7535460992907801}\n",
      "Intel/dynamic_tinybert performance saved: {'exact_match': 0.7535460992907801}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4150d42a19a049f7b44f72a41c458e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of distilbert/distilbert-base-cased-distilled-squad : {'exact_match': 0.7836879432624113}\n",
      "distilbert/distilbert-base-cased-distilled-squad performance saved: {'exact_match': 0.7836879432624113}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3e0a353da64c99bc60be8be85db395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of FabianWillner/distilbert-base-uncased-finetuned-squad : {'exact_match': 0.8368794326241135}\n",
      "FabianWillner/distilbert-base-uncased-finetuned-squad performance saved: {'exact_match': 0.8368794326241135}\n"
     ]
    }
   ],
   "source": [
    "#We are evaluating on our \"train\" dataset in this section\n",
    "datasetEval = 'train'\n",
    "\n",
    "model_name = 'deepset/roberta-base-squad2'\n",
    "evaluate_hf_model(model_name, datasetEval)\n",
    "modelNames.append(model_name)\n",
    "modelPerformanceList.append(modelPerformance)\n",
    "print('{} performance saved: {}'.format(model_name, modelPerformance))\n",
    "\n",
    "model_name = 'Intel/dynamic_tinybert'\n",
    "evaluate_hf_model(model_name, datasetEval)\n",
    "modelNames.append(model_name)\n",
    "modelPerformanceList.append(modelPerformance)\n",
    "print('{} performance saved: {}'.format(model_name, modelPerformance))\n",
    "\n",
    "model_name = 'distilbert/distilbert-base-cased-distilled-squad'\n",
    "evaluate_hf_model(model_name, datasetEval)\n",
    "modelNames.append(model_name)\n",
    "modelPerformanceList.append(modelPerformance)\n",
    "print('{} performance saved: {}'.format(model_name, modelPerformance))\n",
    "\n",
    "model_name = 'FabianWillner/distilbert-base-uncased-finetuned-squad'\n",
    "evaluate_hf_model(model_name, datasetEval)\n",
    "modelNames.append(model_name)\n",
    "modelPerformanceList.append(modelPerformance)\n",
    "print('{} performance saved: {}'.format(model_name, modelPerformance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5a121",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis Models\n",
    "Here we are evaluating our \"Sentiment Analysis\" models like ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967d2bc",
   "metadata": {},
   "source": [
    "### Part 4: Evaluation on Validation Dataset\n",
    "In this section we are evaluating performances for our models on our \"Validation\" dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd599d",
   "metadata": {},
   "source": [
    "#### Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8868b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d81745a2",
   "metadata": {},
   "source": [
    "#### SQuAD Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5649180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2542deb",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f924c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63db1949",
   "metadata": {},
   "source": [
    "### Part 5: Evaluation on Test Dataset\n",
    "In this section we are evaluating performances for our models on our \"Test\" dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1e369",
   "metadata": {},
   "source": [
    "#### Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb390c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41705f93",
   "metadata": {},
   "source": [
    "#### SQuAD Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c7e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9676148a",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa02970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d99b98af",
   "metadata": {},
   "source": [
    "### Part 6: Results Visualization\n",
    "In this section we will be graphing based on model type and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a0940",
   "metadata": {},
   "source": [
    "#### Baseline Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ad1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da597ecc",
   "metadata": {},
   "source": [
    "#### SQuAD Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d5af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d4b5e47",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e46c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e01679f4",
   "metadata": {},
   "source": [
    "### Part 7: Conclusion and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f9a65",
   "metadata": {},
   "source": [
    "Our results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3acd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
