{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "680a9703-47c6-47d4-97e2-6b3c5c0d6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "\n",
    "import os, random, sys, copy\n",
    "import torch, torch.nn as nn, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2981651e-ff6d-4d98-9519-4eb0cf2e0bd2",
   "metadata": {},
   "source": [
    "We must now load the dataset from our local JSON files. \n",
    "\n",
    "For refernece, see https://huggingface.co/docs/datasets/en/loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b74b3a79-e6d1-4eda-903b-64123e120ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Jame's mother has a photo of Jane standing at a height of 14 inches, whereas a mountain appears to have height of 26 cm. It looks that way because? \n",
      "A: the mountain was farther away\n",
      "A: Jane was farther away\n",
      "A: Option 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#Load the dataset\n",
    "#Todo: Split into train/test, right now this is all training\n",
    "squad_dataset = load_dataset(\"json\", data_files=[\"QQA Data/QQA_dev.json\", \"QQA Data/QQA_test.json\", \"QQA Data/QQA_train.json\"])\n",
    "\n",
    "print('Q: ' + dataset['train'][0]['question'])\n",
    "print('A: ' + dataset['train'][0]['Option1'])\n",
    "print('A: ' + dataset['train'][0]['Option2'])\n",
    "print('A: ' + dataset['train'][0]['answer'])\n",
    "\n",
    "#Load evaluation metric\n",
    "accuracy_metric = load(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3baa20a-caaf-4610-924e-676a7be967b6",
   "metadata": {},
   "source": [
    "The imported data is of the form:\n",
    "\n",
    "{\"question\": \"Jame's mother has a photo of Jane standing at a height of 14 inches, whereas a mountain appears to have height of 26 cm. It looks that way because? \", \"Option1\": \"the mountain was farther away\", \"Option2\": \"Jane was farther away\", \"answer\": \"Option 2\", \"type\": \"Type_3\", \"question_sci_10E\": \"Jame's mother has a photo of Jane standing at a height of 1.4000000000E+01 inches, whereas a mountain appears to have height of 2.6000000000E+01 cm. It looks that way because? \", \"question_char\": \"Jame's mother has a photo of Jane standing at a height of 1 4 inches, whereas a mountain appears to have height of 2 6 cm. It looks that way because? \", \"question_sci_10E_char\": \"Jame's mother has a photo of Jane standing at a height of 1 . 4 0 0 0 0 0 0 0 0 0 E + 0 1 inches, whereas a mountain appears to have height of 2 . 6 0 0 0 0 0 0 0 0 0 E + 0 1 cm. It looks that way because? \", \"question_mask\": \"Jame's mother has a photo of Jane standing at a height of [Num] inches, whereas a mountain appears to have height of [Num] cm. It looks that way because? \"\n",
    "\n",
    "For reference, see:\n",
    "https://huggingface.co/docs/transformers/en/tasks/multiple_choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cb7665d-c163-4443-b153-eb258e0d9f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0f7232f6474e059099927c6e7ca2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerformance of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name, squad_evaluate\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mreferences)))\n\u001b[0;32m---> 35\u001b[0m \u001b[43mevaluate_hf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 18\u001b[0m, in \u001b[0;36mevaluate_hf_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m references \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Get predictions, and save corresponding reference (if we were using the whole dataset, we wouldn't need this step)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset_generator(squad_dataset), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(squad_dataset)):\n\u001b[1;32m     20\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m : ex[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_text\u001b[39m\u001b[38;5;124m'\u001b[39m : processor(ex[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     23\u001b[0m     }\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# In each example, there are multiple possible answers which we compare to. Here we are converting from them from the datasets format to the one expected by the evaluation metric. \u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 12\u001b[0m, in \u001b[0;36mevaluate_hf_model.<locals>.dataset_generator\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataset_generator\u001b[39m(dataset):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m (ex,\n\u001b[0;32m---> 12\u001b[0m             {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: ex[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "#Todo\n",
    "\n",
    "model_name = 'deepset/roberta-base-squad2'\n",
    "\n",
    "def evaluate_hf_model(model_name):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)       # Initialize the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)                   # Initialize the tokenizer\n",
    "\n",
    "    processor = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    def dataset_generator(dataset):\n",
    "        for ex in dataset:\n",
    "            yield (ex,\n",
    "                {'question' : ex['question'], 'context': ex['context']})\n",
    "            \n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    # Get predictions, and save corresponding reference (if we were using the whole dataset, we wouldn't need this step)\n",
    "    for ex in tqdm(dataset_generator(squad_dataset), total=len(squad_dataset)):\n",
    "\n",
    "        predictions.append({\n",
    "                'id' : ex[0]['id'],\n",
    "                'prediction_text' : processor(ex[1])['answer']\n",
    "        }\n",
    "        )\n",
    "\n",
    "        # In each example, there are multiple possible answers which we compare to. Here we are converting from them from the datasets format to the one expected by the evaluation metric. \n",
    "        references.append({\n",
    "            'id' : ex[0]['id'],\n",
    "            'answers' : [{'text' : z[0], 'answer_start' : z[1]} for z in zip(ex[0]['answers']['text'], ex[0]['answers']['answer_start'])]\n",
    "        })\n",
    "\n",
    "    # Compute metrics\n",
    "    print('Performance of {} : {}'.format(model_name, squad_evaluate.compute(predictions=predictions, references=references)))\n",
    "\n",
    "evaluate_hf_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f5147-a707-4d31-8bd7-98a2f593ba82",
   "metadata": {},
   "source": [
    "For evaluation, see https://huggingface.co/spaces/evaluate-metric/accuracy\n",
    "\n",
    "For fine-tuning, see: https://huggingface.co/docs/transformers/en/training#train-with-pytorch-trainer\n",
    "\n",
    "For handing multiple choice, see https://huggingface.co/docs/transformers/en/tasks/multiple_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c309978-a036-434e-9bb0-73646fbb2085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
